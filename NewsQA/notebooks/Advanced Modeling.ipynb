{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Now that we have a baseline model, we can move on to the \"Software 2.0\" approach where we train machine learning models. The data pre-processing code can be found in the file 'newsqa.py'. Please refer to that file before looking at this code. \n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "We create a NewsQaExample object for each input example which stores all details like the tokens, index mappings, labels and others. Code referenced for pre-processing: https://github.com/chiayewken/bert-qa.\n",
    "\n",
    "Challenges addressed in preprocessing:\n",
    "\n",
    "- Handling data that exceeds BERT maximum token length. For training data, we use a sliding window to find the part of the text which has the answer and use that as out input. For test data, we use all the windows and get answers for each window.\n",
    "\n",
    "- Maintaining token -> original word indices and word -> character indices.\n",
    "\n",
    "**Modelling:**\n",
    "\n",
    "The training code can be found in 'newsqa.py'. We create a NewsQaModel object which stores the model and handles training and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from newsqa import NewsQaExample, NewsQaModel, create_dataset, get_single_prediction\n",
    "import utils\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "NEWS_STORIES = utils.open_pickle('../data/news_stories.pkl')\n",
    "data = pd.read_csv('../data/newsqa-dataset-cleaned.csv')\n",
    "total_examples = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples():\n",
    "    '''\n",
    "    Return a list of NewsQaExample objects\n",
    "    '''\n",
    "    # If a pickle file exists for examples, read the file\n",
    "    if os.path.isfile('../data/examples.pkl'):\n",
    "        return utils.open_pickle('../data/examples.pkl')\n",
    "    \n",
    "    examples = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        ex = NewsQaExample(NEWS_STORIES[row['story_id']], row['question'], row['start_idx'], row['end_idx'])\n",
    "        examples.append(ex)\n",
    "        utils.drawProgressBar(idx + 1, total_examples)\n",
    "    \n",
    "    print('\\n')\n",
    "    # Saving examples to a pickle file\n",
    "    utils.save_pickle('../data/examples.pkl', examples)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(examples, tokenizer_name):\n",
    "    '''\n",
    "    Returns train, val and test datasets from examples\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    examples: list\n",
    "              A list of NewsQaExample objects\n",
    "              \n",
    "    tokenizer_name: str\n",
    "                    The tokenizer to use\n",
    "    '''\n",
    "    model_name = tokenizer_name.split('-')[0]\n",
    "    \n",
    "    if os.path.isfile('../data/dataset_' + model_name + '.pkl'):\n",
    "        return utils.open_pickle('../data/dataset_' + model_name + '.pkl')\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    if tokenizer_name == 'bert-large-uncased-whole-word-masking-finetuned-squad':\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    if tokenizer_name == 'distilbert-base-uncased-distilled-squad':\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    print(\"Getting input features:\")\n",
    "    for idx, ex in enumerate(examples):\n",
    "        input_features = ex.encode_plus(tokenizer, pad = True)\n",
    "        features.append(input_features)\n",
    "        labels.append(ex.get_label())\n",
    "        utils.drawProgressBar(idx + 1, total_examples)\n",
    "    \n",
    "    # Getting TensorDataset\n",
    "    train_set, val_set, test_set, feature_idx_map = create_dataset(features, labels, model = model_name)\n",
    "    # Saving the dataset in a file\n",
    "    utils.save_pickle('../data/dataset_' + model_name + '.pkl', (train_set, val_set, test_set, feature_idx_map))\n",
    "    \n",
    "    return (train_set, val_set, test_set, feature_idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_set, val_set, test_set, batch_size):\n",
    "    '''\n",
    "    Creates torch dataloaders for train, validation and test sets\n",
    "    '''\n",
    "    train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, \n",
    "                          sampler = RandomSampler(train_set))\n",
    "\n",
    "    val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, \n",
    "                            sampler = SequentialSampler(val_set))\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, \n",
    "                             sampler = SequentialSampler(test_set))\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model_name, train_loader, val_loader, feature_idx_map, device, \n",
    "                   epochs = 1, learning_rate = 1e-5):\n",
    "    '''\n",
    "    Fine-tunes a pretrained model\n",
    "    '''\n",
    "    if model_name == 'bert-large-uncased-whole-word-masking-finetuned-squad':\n",
    "        model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "        # Freezing bert parameters\n",
    "        for param in model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if model_name == 'distilbert-base-uncased-distilled-squad':\n",
    "        model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "        # Freezing distilbert parameters\n",
    "        for param in model.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    short_name = model_name.split('-')[0]\n",
    "    \n",
    "    newsqa_model = NewsQaModel(model)\n",
    "    newsqa_model.train(train_loader, val_loader, feature_idx_map, device, \n",
    "                       num_epochs = epochs, lr = learning_rate, \n",
    "                       filename = '../data/' + short_name + '_model.pt')\n",
    "    \n",
    "    return newsqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of NewsQaExample objects\n",
    "examples = get_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained-BERT\n",
    "\n",
    "We use the bert-large-uncased-whole-word-masking-finetuned-squad pretrained model which is trained specifically for question answering task and will be quick to fine-tune on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name\n",
    "bert_model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the training, validation and test sets\n",
    "bert_datasets = get_datasets(examples, bert_model_name)\n",
    "bert_train_set, bert_val_set, bert_test_set, bert_feature_idx_map = bert_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "bert_loaders = get_dataloaders(bert_train_set, bert_val_set, bert_test_set, batch_size = BATCH_SIZE)\n",
    "bert_train_loader, bert_val_loader, bert_test_loader = bert_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      2\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 4\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_feature_idx_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mfinetune_model\u001b[1;34m(model_name, train_loader, val_loader, feature_idx_map, device, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     18\u001b[0m short_name \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     20\u001b[0m newsqa_model \u001b[38;5;241m=\u001b[39m NewsQaModel(model)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mnewsqa_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_idx_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshort_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m newsqa_model\n",
      "File \u001b[1;32m~\\OneDrive - The University of Hong Kong - Connect\\MStat Studies\\STAT8021\\03 Project\\NewsQA\\notebooks\\..\\newsqa.py:486\u001b[0m, in \u001b[0;36mNewsQaModel.train\u001b[1;34m(self, training_set, eval_set, feature_idx_map, device, optimizer, num_epochs, lr, filename)\u001b[0m\n\u001b[0;32m    484\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_features)\n\u001b[0;32m    485\u001b[0m loss, start_scores, end_scores \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m--> 486\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m                                \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_positions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m                                \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_positions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m total_f1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    492\u001b[0m total_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\OneDrive - The University of Hong Kong - Connect\\MStat Studies\\STAT8021\\03 Project\\NewsQA\\notebooks\\..\\newsqa.py:626\u001b[0m, in \u001b[0;36mNewsQaModel.calculate_metrics\u001b[1;34m(self, start_scores, end_scores, start_idx, end_idx)\u001b[0m\n\u001b[0;32m    623\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m end_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# .cpu().numpy() + 1\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# Get the predicted indices from scores\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m pred_start \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m pred_end \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(end_scores, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    629\u001b[0m f1_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\.conda\\envs\\pythonProject\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pythonProject\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pythonProject\\lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "bert_model = finetune_model(bert_model_name, bert_train_loader, bert_val_loader, bert_feature_idx_map, \n",
    "                            device, epochs = EPOCHS, learning_rate = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [====================] 549/549\n",
      "loss: 3.3887\tf1:0.3313\tacc:0.5250\n"
     ]
    }
   ],
   "source": [
    "# Evaluation the performance on test set\n",
    "bert_model.load('../data/bert_model.pt')\n",
    "bert_eval_metrics = bert_model.evaluate(bert_test_loader, bert_feature_idx_map, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [====================] 549/549\n",
      "loss: 6.0508\tf1:0.2614\tacc:0.4329\n"
     ]
    }
   ],
   "source": [
    "# Evalutating performance on the model without fine-tuining\n",
    "bert_non_finetuned = BertForQuestionAnswering.from_pretrained(bert_model_name)\n",
    "bert_non_finetuned.to(device)\n",
    "\n",
    "bert_newsqa_model = NewsQaModel(bert_non_finetuned)\n",
    "\n",
    "non_finetuned_eval_metrics = bert_newsqa_model.evaluate(bert_test_loader, bert_feature_idx_map, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained-DistilBERT\n",
    "\n",
    "We use the distilbert-base-uncased-distilled-squad pretrained model which is trained specifically for question answering task and will be quick to fine-tune on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model name\n",
    "dbert_model_name = 'distilbert-base-uncased-distilled-squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the training, validation and test sets\n",
    "dbert_datasets = get_datasets(examples, dbert_model_name)\n",
    "dbert_train_set, dbert_val_set, dbert_test_set, dbert_feature_idx_map = dbert_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "dbert_loaders = get_dataloaders(dbert_train_set, dbert_val_set, dbert_test_set, batch_size = BATCH_SIZE)\n",
    "dbert_train_loader, dbert_val_loader, dbert_test_loader = dbert_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Progress: [====================] 1921/1921  11m 16s\tloss: 3.9230\tf1: 0.2578\tacc: 0.3777\tval_loss: 3.7380\tval_f1: 0.2780\tval_acc: 0.3965\n",
      "Validation accuracy increased from 0.0000 to 0.3965, saving to models/distilbert.pt\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/5:\n",
      "Progress: [====================] 1921/1921  11m 16s\tloss: 3.8655\tf1: 0.2588\tacc: 0.3855\tval_loss: 3.7188\tval_f1: 0.2911\tval_acc: 0.4216\n",
      "Validation accuracy increased from 0.3965 to 0.4216, saving to models/distilbert.pt\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3/5:\n",
      "Progress: [====================] 1921/1921  11m 16s\tloss: 3.8613\tf1: 0.2589\tacc: 0.3875\tval_loss: 3.7073\tval_f1: 0.2893\tval_acc: 0.4138\n",
      "\n",
      "\n",
      "Epoch 4/5:\n",
      "Progress: [====================] 1921/1921  11m 16s\tloss: 3.8633\tf1: 0.2590\tacc: 0.3881\tval_loss: 3.7228\tval_f1: 0.2882\tval_acc: 0.4121\n",
      "\n",
      "\n",
      "Epoch 5/5:\n",
      "Progress: [====================] 1921/1921  11m 16s\tloss: 3.8660\tf1: 0.2593\tacc: 0.3864\tval_loss: 3.7113\tval_f1: 0.2896\tval_acc: 0.4192"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "dbert_model = finetune_model(dbert_model_name, dbert_train_loader, dbert_val_loader, dbert_feature_idx_map, \n",
    "                             device, epochs = EPOCHS, learning_rate = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [====================] 549/549\n",
      "loss: 3.6821\tf1:0.3028\tacc:0.4342\n"
     ]
    }
   ],
   "source": [
    "# Evaluation the performance on test set\n",
    "dbert_model.load('../data/distilbert_model.pt')\n",
    "dbert_eval_metrics = dbert_model.evaluate(dbert_test_loader, dbert_feature_idx_map, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [====================] 549/549\n",
      "loss: 6.4680\tf1:0.2837\tacc:0.4062\n"
     ]
    }
   ],
   "source": [
    "# Evalutating performance on the model without fine-tuining\n",
    "dbert_non_finetuned = DistilBertForQuestionAnswering.from_pretrained(dbert_model_name)\n",
    "dbert_non_finetuned.to(device)\n",
    "\n",
    "dbert_newsqa_model = NewsQaModel(dbert_non_finetuned)\n",
    "\n",
    "non_finetuned_eval_metrics = dbert_newsqa_model.evaluate(dbert_test_loader, dbert_feature_idx_map, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The summary of the model performances on **test data**\n",
    "\n",
    "<p style=\"text-align: center; font-size: large; font-weight: bold;\"> BERT </p>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> </th>\n",
    "        <th> Loss </th>\n",
    "        <th> F1-score </th>\n",
    "        <th> Accuracy </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Pre-trained model </th>\n",
    "        <td> 6.0508 </td>\n",
    "        <td> 0.2614 </td>\n",
    "        <td> 0.4329 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Fine-tuned model </th>\n",
    "        <td> 3.3887 </td>\n",
    "        <td> 0.3313 </td>\n",
    "        <td> 0.5250 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align: center; font-size: large; font-weight: bold;\"> DistilBERT </p>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> </th>\n",
    "        <th> Loss </th>\n",
    "        <th> F1-score </th>\n",
    "        <th> Accuracy </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Pre-trained model </th>\n",
    "        <td> 6.4680 </td>\n",
    "        <td> 0.2837 </td>\n",
    "        <td> 0.4062 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Fine-tuned model </th>\n",
    "        <td> 3.6821 </td>\n",
    "        <td> 0.3028 </td>\n",
    "        <td> 0.4342 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "In terms of accuracy, the DistilBERT fine-tuned model does almost the same as the original pre-trained BERT model. Overall too, the BERT model does better than DistilBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "The deployment involves two stages:\n",
    "1. Back-end deployment\n",
    "2. Front-end deployment\n",
    "\n",
    "The **back-end** deployment was done using FastAPI. The API has two functions: one that returns an article from the dataset when you provide a key, and another that returns the predicted answer character ranges and answer text when you provide the article and a question. The BERT model is used for predictions. The article is divided into several parts using a sliding window and predictions for each window is returned. This API was then deployed to google cloud platform app engine. You can try it out here [https://fastapi-newsqa.wl.r.appspot.com/docs](https://fastapi-newsqa.wl.r.appspot.com/docs).\n",
    "<br><br>\n",
    "\n",
    "Some examples to try out on the API, you can get the article text using the key:<br>\n",
    "{key: 3cb1efaccb2bdf73ffdaa14abf7a145d47dc690d, question: Who will play a key role?}<br>\n",
    "{key: 32e4f6613c739bdf2c1b9a4d85ea75ec2d5017ad, question: Who owns the services?}<br>\n",
    "{key: e1aa3cc0557bc36c8bdb78a78bc24e1770db05cc, question: Where was Howie Mandel when he fell ill?}<br>\n",
    "<br><br>\n",
    "\n",
    "A simple **front-end** UI was built using HTML and Bootstrap which makes jQuery AJAX calls to the back-end URL. The UI has some examples that can be used to test the API. When you select an example, an API call is made to the get_article function when returns the example article. Instead of selecting an example, you cam also paste your own text and question. Then when you click submit, another API call is made to get the predictions. The predicted answer character ranges are highlighted in th article and the answer is displayed. This UI was deployed on GitHub pages, on this URL [smitkiri.github.io/newsqa](https://smitkiri.github.io/newsqa)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pythonProject]",
   "language": "python",
   "name": "conda-env-.conda-pythonProject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
